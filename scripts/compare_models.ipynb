{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Base vs Trained Models\n",
        "\n",
        "This notebook loads helper functions from `scripts/compare_models.py` and lets you\n",
        "interactively configure models, prompts, and evaluation settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and helpers\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from scripts.compare_models import (\n",
        "    CompareConfig,\n",
        "    load_model_and_tokenizer,\n",
        "    generate_text,\n",
        "    compute_logprobs,\n",
        ")\n",
        "\n",
        "# Configure here\n",
        "cfg = CompareConfig(\n",
        "    base_model=\"/path/to/original\",\n",
        "    trained_model=\"/path/to/trained\",\n",
        "    prompts_path=None,  # or a path to .jsonl/.json with {\"prompt\": ...}\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=0,\n",
        "    device=\"auto\",\n",
        "    dtype=\"float16\",\n",
        "    batch_size=8,\n",
        "    eos_token=None,\n",
        ")\n",
        "\n",
        "# Load prompts\n",
        "if cfg.prompts_path is None:\n",
        "    prompts = [\n",
        "        \"Write a short poem about the sea.\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain the concept of reinforcement learning in one paragraph.\",\n",
        "        \"Translate to Spanish: 'Good morning, how are you?'\",\n",
        "    ]\n",
        "else:\n",
        "    p = Path(cfg.prompts_path)\n",
        "    if p.suffix == \".jsonl\":\n",
        "        prompts = [json.loads(line)[\"prompt\"] for line in p.open()]\n",
        "    else:  # .json\n",
        "        items = json.load(p.open())\n",
        "        prompts = [item[\"prompt\"] if isinstance(item, dict) else item for item in items]\n",
        "\n",
        "# Load models\n",
        "base_model, base_tok = load_model_and_tokenizer(cfg.base_model, cfg.device, cfg.dtype)\n",
        "trained_model, trained_tok = load_model_and_tokenizer(cfg.trained_model, cfg.device, cfg.dtype)\n",
        "\n",
        "# Optional: enforce same eos_token\n",
        "if cfg.eos_token is not None:\n",
        "    base_tok.eos_token = cfg.eos_token\n",
        "    trained_tok.eos_token = cfg.eos_token\n",
        "\n",
        "# Generate\n",
        "base_outputs = generate_text(base_model, base_tok, prompts, cfg)\n",
        "trained_outputs = generate_text(trained_model, trained_tok, prompts, cfg)\n",
        "\n",
        "# Compute log-probs of generated responses under each model\n",
        "base_logps = compute_logprobs(base_model, base_tok, prompts, base_outputs, cfg)\n",
        "trained_logps = compute_logprobs(trained_model, trained_tok, prompts, trained_outputs, cfg)\n",
        "\n",
        "# Aggregate per-sample metrics\n",
        "rows = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "    base_nonzero = (base_logps[i] != 0).sum().clamp(min=1).item()\n",
        "    trained_nonzero = (trained_logps[i] != 0).sum().clamp(min=1).item()\n",
        "    rows.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"base_text\": base_outputs[i],\n",
        "        \"trained_text\": trained_outputs[i],\n",
        "        \"base_mean_logp\": float(base_logps[i].sum().item() / base_nonzero),\n",
        "        \"trained_mean_logp\": float(trained_logps[i].sum().item() / trained_nonzero),\n",
        "    })\n",
        "\n",
        "rows[:2]  # preview\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
