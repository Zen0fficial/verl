{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Base vs Trained Models\n",
        "\n",
        "This notebook loads helper functions from `scripts/compare_models.py` and lets you\n",
        "interactively configure models, prompts, and evaluation settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and helpers\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from scripts.compare_models import (\n",
        "    CompareConfig,\n",
        "    load_model_and_tokenizer,\n",
        "    generate_text,\n",
        "    compute_logprobs,\n",
        ")\n",
        "\n",
        "# Configure here\n",
        "cfg = CompareConfig(\n",
        "    base_model=\"/path/to/original\",\n",
        "    trained_model=\"/path/to/trained\",\n",
        "    prompts_path=None,  # or a path to .jsonl/.json with {\"prompt\": ...}\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    top_k=0,\n",
        "    device=\"auto\",\n",
        "    dtype=\"float16\",\n",
        "    batch_size=8,\n",
        "    eos_token=None,\n",
        ")\n",
        "\n",
        "use_vllm = True\n",
        "vllm_tp_size = 1\n",
        "\n",
        "# Load prompts\n",
        "if cfg.prompts_path is None:\n",
        "    prompts = [\n",
        "        \"Write a short poem about the sea.\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain the concept of reinforcement learning in one paragraph.\",\n",
        "        \"Translate to Spanish: 'Good morning, how are you?'\",\n",
        "    ]\n",
        "else:\n",
        "    p = Path(cfg.prompts_path)\n",
        "    if p.suffix == \".jsonl\":\n",
        "        prompts = [json.loads(line)[\"prompt\"] for line in p.open()]\n",
        "    else:  # .json\n",
        "        items = json.load(p.open())\n",
        "        prompts = [item[\"prompt\"] if isinstance(item, dict) else item for item in items]\n",
        "\n",
        "if use_vllm:\n",
        "    # vLLM-based generation\n",
        "    base_outputs = vllm_generate(\n",
        "        cfg.base_model, prompts, cfg.max_new_tokens, cfg.temperature, cfg.top_p, cfg.top_k, cfg.eos_token, vllm_tp_size\n",
        "    )\n",
        "    trained_outputs = vllm_generate(\n",
        "        cfg.trained_model, prompts, cfg.max_new_tokens, cfg.temperature, cfg.top_p, cfg.top_k, cfg.eos_token, vllm_tp_size\n",
        "    )\n",
        "    # For log-probs, reuse HF forward on same models for now\n",
        "    base_model, base_tok = load_model_and_tokenizer(cfg.base_model, cfg.device, cfg.dtype)\n",
        "    trained_model, trained_tok = load_model_and_tokenizer(cfg.trained_model, cfg.device, cfg.dtype)\n",
        "else:\n",
        "    # HF generation path\n",
        "    base_model, base_tok = load_model_and_tokenizer(cfg.base_model, cfg.device, cfg.dtype)\n",
        "    trained_model, trained_tok = load_model_and_tokenizer(cfg.trained_model, cfg.device, cfg.dtype)\n",
        "    base_outputs = generate_text(base_model, base_tok, prompts, cfg)\n",
        "    trained_outputs = generate_text(trained_model, trained_tok, prompts, cfg)\n",
        "\n",
        "# Compute log-probs of generated responses under each model\n",
        "base_logps = compute_logprobs(base_model, base_tok, prompts, base_outputs, cfg)\n",
        "trained_logps = compute_logprobs(trained_model, trained_tok, prompts, trained_outputs, cfg)\n",
        "\n",
        "# Aggregate per-sample metrics\n",
        "rows = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "    base_nonzero = (base_logps[i] != 0).sum().clamp(min=1).item()\n",
        "    trained_nonzero = (trained_logps[i] != 0).sum().clamp(min=1).item()\n",
        "    rows.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"base_text\": base_outputs[i],\n",
        "        \"trained_text\": trained_outputs[i],\n",
        "        \"base_mean_logp\": float(base_logps[i].sum().item() / base_nonzero),\n",
        "        \"trained_mean_logp\": float(trained_logps[i].sum().item() / trained_nonzero),\n",
        "    })\n",
        "\n",
        "rows[:2]  # preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vLLM generation utilities\n",
        "try:\n",
        "    from vllm import LLM, SamplingParams\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"vLLM is not installed. Please install vLLM to use this cell.\") from e\n",
        "\n",
        "\n",
        "def vllm_generate(model_path: str, prompts: list[str], max_new_tokens: int, temperature: float, top_p: float, top_k: int, eos_token: str | None = None, tensor_parallel_size: int = 1):\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k if top_k > 0 else None,\n",
        "        max_tokens=max_new_tokens,\n",
        "        stop=[eos_token] if eos_token else None,\n",
        "    )\n",
        "    llm = LLM(model=model_path, tensor_parallel_size=tensor_parallel_size)\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "    return [o.outputs[0].text for o in outputs]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
